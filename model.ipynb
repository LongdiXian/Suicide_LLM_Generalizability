{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3836ff0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21483db2",
   "metadata": {},
   "source": [
    "# Exploring the Generalizability and Explainability of LLMs in Detecting Suicidal Ideation: The Impact of Data Heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c5b22",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c711d7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the combined metadata CSV\n",
    "data = pd.read_csv('combine.csv')\n",
    "\n",
    "# Initialize a list to store processed results\n",
    "results = []\n",
    "\n",
    "# Iterate through each case in the metadata\n",
    "for _, row in data.iterrows():\n",
    "    case = row['case']\n",
    "    label = row['label']\n",
    "    filepath = f'Y:/Rong/transcripts/HAMD/translated_can_HAMD/{case}.csv'\n",
    "\n",
    "    # Check if the transcript file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Missing file: {filepath}\")\n",
    "        results.append({\n",
    "            'case': case,\n",
    "            'question_number': 'H11',\n",
    "            'text': np.nan,\n",
    "            'interviewee_text': np.nan,\n",
    "            'label': label\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Load the transcript CSV\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Possible column names for question number, content, and speaker\n",
    "    qn_candidates = ['question number', 'questionnumber', 'Question Number', 'QuestionNumber']\n",
    "    content_candidates = ['content', 'Content']\n",
    "    speaker_candidates = ['speaker(interviewer/interviewee)', 'Speaker(interviewer/interviewee)', 'speaker']\n",
    "\n",
    "    # Helper function to find the first matching column from candidates\n",
    "    def find_col(candidates):\n",
    "        for c in candidates:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    qn_col = find_col(qn_candidates)\n",
    "    content_col = find_col(content_candidates)\n",
    "    speaker_col = find_col(speaker_candidates)\n",
    "\n",
    "    # If any required column is missing, skip this case\n",
    "    if None in [qn_col, content_col, speaker_col]:\n",
    "        print(f\"Missing column in file: {filepath}\")\n",
    "        results.append({\n",
    "            'case': case,\n",
    "            'question_number': 'H11',\n",
    "            'text': np.nan,\n",
    "            'interviewee_text': np.nan,\n",
    "            'label': label\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Identify indices where question numbers exist\n",
    "    qn_indices = df[df[qn_col].notna()].index.tolist()\n",
    "    h11_idx = None\n",
    "    for idx in qn_indices:\n",
    "        if str(df.loc[idx, qn_col]).strip() == 'H11':\n",
    "            h11_idx = idx\n",
    "            break\n",
    "\n",
    "    # Extract H11 section if it exists\n",
    "    if h11_idx is not None:\n",
    "        # Find the start of the next question number after H11\n",
    "        next_indices = [i for i in qn_indices if i > h11_idx]\n",
    "        end_idx = next_indices[0] if next_indices else len(df)\n",
    "        chunk = df.iloc[h11_idx:end_idx]\n",
    "\n",
    "        # Concatenate all text in H11 section\n",
    "        full_text = ' '.join(chunk[content_col].astype(str))\n",
    "        # Extract only interviewee's text\n",
    "        interviewee_text = ' '.join(\n",
    "            chunk[chunk[speaker_col].str.lower() == 'interviewee'][content_col].astype(str)\n",
    "        )\n",
    "    else:\n",
    "        full_text = np.nan\n",
    "        interviewee_text = np.nan\n",
    "\n",
    "    # Append the processed data for this case\n",
    "    results.append({\n",
    "        'case': case,\n",
    "        'question_number': 'H11',\n",
    "        'text': full_text,\n",
    "        'interviewee_text': interviewee_text,\n",
    "        'label': label\n",
    "    })\n",
    "\n",
    "# Save the final processed dataset\n",
    "final_df = pd.DataFrame(results)\n",
    "final_df.to_csv('H11_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d7781",
   "metadata": {},
   "source": [
    "## Model Training All data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab235ac5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AdamW, get_scheduler\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Seed everything for reproducibility\n",
    "# -----------------------------\n",
    "def seed_everything(seed=6):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset class with chunking\n",
    "# -----------------------------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, overlap=128, cases=None):\n",
    "        self.samples = []\n",
    "        for idx, (text, label) in enumerate(zip(texts, labels)):\n",
    "            case_id = cases[idx] if cases is not None else idx\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"]\n",
    "            start = 0\n",
    "            while start < len(input_ids):\n",
    "                end = start + max_len\n",
    "                chunk = input_ids[start:end]\n",
    "                if len(chunk) < max_len:\n",
    "                    chunk += [tokenizer.pad_token_id] * (max_len - len(chunk))\n",
    "                self.samples.append({\n",
    "                    \"input_ids\": chunk,\n",
    "                    \"label\": label,\n",
    "                    \"case_id\": case_id,\n",
    "                    \"chunk_id\": start // (max_len - overlap)\n",
    "                })\n",
    "                if end >= len(input_ids):\n",
    "                    break\n",
    "                start += max_len - overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long),\n",
    "            \"case_id\": sample[\"case_id\"],\n",
    "            \"chunk_id\": sample[\"chunk_id\"],\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# Custom classifier for base model\n",
    "# -----------------------------\n",
    "class CustomClassifier(torch.nn.Module):\n",
    "    def __init__(self, base_model, hidden_size, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(self.dropout(pooled_output))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return type('Output', (), {'loss': loss, 'logits': logits})\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics calculation\n",
    "# -----------------------------\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"F1\": f1,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV\": precision,\n",
    "        \"NPV\": npv\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Convert dataset back to DataFrame for majority voting\n",
    "# -----------------------------\n",
    "def dataset_to_dataframe(dataset, tokenizer):\n",
    "    records = []\n",
    "    for sample in dataset:\n",
    "        input_ids = sample[\"input_ids\"]\n",
    "        text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        records.append({\n",
    "            \"case_id\": sample[\"case_id\"],\n",
    "            \"chunk_id\": sample[\"chunk_id\"],\n",
    "            \"label\": sample[\"label\"].item(),\n",
    "            \"chunk_text\": text\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# -----------------------------\n",
    "# Training one epoch\n",
    "# -----------------------------\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate model on validation set\n",
    "# -----------------------------\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    true_labels, pred_probs, cased = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_probs.extend(probs.cpu().numpy())\n",
    "            cased.extend(batch[\"case_id\"])\n",
    "    return np.array(cased), np.array(true_labels), np.array(pred_probs)\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop: load models, datasets, and train with early stopping\n",
    "# -----------------------------\n",
    "models = ['indiejoseph/bert-base-cantonese']\n",
    "all_model_metrics = []\n",
    "model_dir = \"./saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "for model_name in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    data1 = pd.read_csv(\"./H14_used.csv\")\n",
    "    # data1 = data1[data1['question_number'] == 'H11']\n",
    "    data1['label'] = data1['suicidal'].map({'no suicidal': 0, 'passive': 1, 'active': 1})\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    y = data1['label']\n",
    "    X = data1['text']\n",
    "    all_folds_results=[]\n",
    "    \n",
    "    folds = list(skf.split(X, y))\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        print(f\"\\n--- Fold {fold_idx} ---\")\n",
    "        train_data = data1.iloc[train_idx].reset_index(drop=True)\n",
    "        val_data = data1.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        train_texts = train_data['text'].tolist()\n",
    "        train_labels = train_data['label'].tolist()\n",
    "        val_texts = val_data['text'].tolist()\n",
    "        val_labels = val_data['label'].tolist()\n",
    "        val_cases = val_data[\"case\"].tolist()\n",
    "        \n",
    "        train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = TextDataset(val_texts, val_labels, tokenizer, cases=val_cases)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        # Load model\n",
    "        try:\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "        except:\n",
    "            base = AutoModel.from_pretrained(model_name)\n",
    "            model = CustomClassifier(base_model=base, hidden_size=base.config.hidden_size).to(device)\n",
    "        \n",
    "        optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "        scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*30)\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        # -----------------------------\n",
    "        # Training loop with early stopping\n",
    "        # -----------------------------\n",
    "        patience = 5\n",
    "        counter = 0\n",
    "        best_auc = 0.0\n",
    "        \n",
    "        for epoch in range(30):\n",
    "            print(f\"Epoch {epoch + 1}\")\n",
    "            train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, scaler)\n",
    "            print(f\"Train loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            cased, true_labels, pred_probs = eval_model(model, val_loader, device)\n",
    "            val_chunks = dataset_to_dataframe(val_dataset, tokenizer)\n",
    "            fold_results = pd.DataFrame({\n",
    "                \"cased\": val_chunks[\"case_id\"],\n",
    "                \"true_label\": val_chunks[\"label\"],\n",
    "                \"prob0\": pred_probs[:, 0],\n",
    "                \"prob1\": pred_probs[:, 1],\n",
    "            })\n",
    "            \n",
    "            # Majority voting per case\n",
    "            voting_df = (\n",
    "                fold_results\n",
    "                .assign(pred=(fold_results[\"prob1\"] > 0.5).astype(int))\n",
    "                .groupby(\"cased\")\n",
    "                .agg({\n",
    "                    \"true_label\": \"first\",\n",
    "                    \"pred\": lambda x: x.value_counts().idxmax(),\n",
    "                    \"prob1\": \"mean\"\n",
    "                })\n",
    "                .reset_index()\n",
    "                .rename(columns={\"cased\": \"case\", \"true_label\": \"label\", \"prob1\": \"prob\"})\n",
    "            )\n",
    "            \n",
    "            y_true = voting_df[\"label\"]\n",
    "            y_pred = voting_df[\"pred\"]\n",
    "            y_prob = voting_df[\"prob\"]\n",
    "            \n",
    "            metrics = compute_metrics(y_true, y_prob, y_pred)\n",
    "            current_auc = metrics[\"AUC\"]\n",
    "            print(f\"Validation AUC: {current_auc:.4f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if current_auc > best_auc:\n",
    "                best_auc = current_auc\n",
    "                counter = 0\n",
    "                # Save best model per fold\n",
    "                torch.save(model.state_dict(), f\"{model_dir}/{model_name.split('/')[-1]}_fold{fold_idx}_best.pt\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        # Compute p-value for Mann-Whitney U test\n",
    "        pos_probs = y_prob[y_true == 1]\n",
    "        neg_probs = y_prob[y_true == 0]\n",
    "        if (len(pos_probs) > 0) and (len(neg_probs) > 0):\n",
    "            u_stat, p_value = mannwhitneyu(pos_probs, neg_probs, alternative='two-sided')\n",
    "        else:\n",
    "            p_value = np.nan\n",
    "        \n",
    "        metrics[\"Model\"] = model_name.split(\"/\")[-1]\n",
    "        metrics[\"Question\"] = 'H14'\n",
    "        metrics[\"pvalue\"] = p_value\n",
    "        all_model_metrics.append(metrics)\n",
    "        all_folds_results.append(fold_results)\n",
    "\n",
    "# -----------------------------\n",
    "# Save metrics summary\n",
    "# -----------------------------\n",
    "metrics_df = pd.DataFrame(all_model_metrics)\n",
    "metrics_df.to_csv(\"./H14Alldata_model_metrics_summary.csv\", index=False)\n",
    "# metrics_df.to_csv(\"./H11_model_metrics_summary.csv\", index=False)\n",
    "print(metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
